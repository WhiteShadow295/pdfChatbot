{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain_community.llms.huggingface_hub import HuggingFaceHub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import T5Tokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load env + llm + embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "# llm\n",
    "llm = HuggingFaceHub(\n",
    "            repo_id=\"google/flan-t5-large\",\n",
    "            model_kwargs={\"temperature\": 0.5, \"max_length\": 512}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/multi-qa-MiniLM-L6-cos-v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(pdf_path):\n",
    "    try:\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        return documents\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "# File Path\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "pdf_file = os.path.join(parent_dir, \"PDF\", \"AnonCred-RWC.pdf\")\n",
    "\n",
    "\n",
    "pdf_text = read_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove PUA (Private Use Area) characters from text\n",
    "def remove_pua(text):\n",
    "    # Remove PUA from BMP\n",
    "    text = re.sub(r'[\\ue000-\\uf8ff]', '', text)\n",
    "\n",
    "    # Remove PUA from Supplementary Area A\n",
    "    text = re.sub(r'[\\U000f0000-\\U000ffffd]', '', text)\n",
    "\n",
    "    # Remove PUA from Supplementary Area B\n",
    "    text = re.sub(r'[\\U00100000-\\U0010fffd]', '', text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pdf_text)):\n",
    "    pdf_text[i].page_content = pdf_text[i].page_content.replace(\"-\\n\", \"\") # remove hyphenation\n",
    "    pdf_text[i].page_content = pdf_text[i].page_content.replace(\"\\n\", \" \") # remove new lines\n",
    "    pdf_text[i].page_content = remove_pua(pdf_text[i].page_content ) # remove PUA characters\n",
    "    pdf_text[i].page_content = re.sub(r'\\s+', ' ', pdf_text[i].page_content) # remove extra spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer done\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\", use_fast=True, legacy=False)\n",
    "\n",
    "# split text into chunks\n",
    "def flan_t5_len(text):\n",
    "    return len(tokenizer.encode(text, truncation=False))\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=60,\n",
    "    length_function=flan_t5_len\n",
    ")\n",
    "\n",
    "text_splitter.split_documents(pdf_text)\n",
    "\n",
    "print(\"tokenizer done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x20717778470>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore = FAISS.from_documents(pdf_text, embeddings)\n",
    "vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\":7},\n",
    "    search_type=\"mmr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationalRetrievalChain(verbose=False, combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\"), llm=HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-large', timeout=None)>, repo_id='google/flan-t5-large', task='text2text-generation', model_kwargs={'temperature': 0.5, 'max_length': 512}), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context'), question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-large', timeout=None)>, repo_id='google/flan-t5-large', task='text2text-generation', model_kwargs={'temperature': 0.5, 'max_length': 512}), output_parser=StrOutputParser(), llm_kwargs={}), return_source_documents=True, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020717778470>, search_type='mmr', search_kwargs={'k': 7}))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the verbose parameter to True to see the more details\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever, return_source_documents=True, verbose=False)\n",
    "qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this paper we describe a practical digital identity project of a global scale, which solves a number of privacy and scalability problems using the concepts of anonymous credentials and permissioned blockchains.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what is the main concept of this paper?\"\n",
    "\n",
    "result = qa_chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"chat_history\": chat_history\n",
    "        })\n",
    "\n",
    "chat_history.append((question, result[\"answer\"]))\n",
    "\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pdfchatbot-X5Jm-f5u-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
